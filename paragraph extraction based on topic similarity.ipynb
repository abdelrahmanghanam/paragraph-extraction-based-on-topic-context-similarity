{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ae9beb4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h1> PDF paragraphs extraction based on topic similarity </h1>\n",
    "<b> this code is for text analysis. basically, it takes a pdf file extracts paragraphs, after that the paragraphs are extracted based on its similarity to a specified topic, finally each paragraph is given a headline based on most important sentence in the extracted paragraph </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bed40e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3> <ol>1. paragraph extraction </ol></h3>\n",
    "<b> in the beggining we need to take a pdf file, analyze it and extract paragraphs. the challenge here is that regular paragraphs has a '\\n' as delimeter, but it is not the case in pdf files as it each line ends with '\\n' so to overcome this issue I splitted all lines of the pdf file and used empty lines as an end to a praragraph </b>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "283b060f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pymupdf in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (1.23.7)\n",
      "Requirement already satisfied: PyMuPDFb==1.23.7 in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (from pymupdf) (1.23.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8ec552cf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "37\n",
      "32\n",
      "38\n",
      "26\n",
      "44\n",
      "43\n",
      "35\n",
      "24\n",
      "40\n",
      "42\n",
      "44\n",
      "25\n",
      "43\n",
      "41\n",
      "64\n",
      "10\n",
      "there are \n",
      "Paragraph 1: See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/332451019 A Study of Sentiment Analysis: Concepts, Techniques, and Challenges Chapter · January 2019 DOI: 10.1007/978-981-13-6459-4_16 CITATIONS 7 READS 10,422 3 authors, including: Dr. Manjula Bairam Kakatiya University 33 PUBLICATIONS   96 CITATIONS SEE PROFILE R Lakshman Naik KU College of Engineering and Technology, Kakatiya University 26 PUBLICATIONS   170 CITATIONS SEE PROFILE All content following this page was uploaded by R Lakshman Naik on 06 January 2020. The user has requested enhancement of the downloaded file.\n",
      "----------------------------------\n",
      "Paragraph 2: A Study of Sentiment Analysis: Concepts, Techniques, and Challenges Ameen Abdullah Qaid Aqlan, B. Manjula and R. Lakshman Naik Abstract Sentiment analysis (SA) is a process of extensive exploration of data stored on the Web to identify and categorize the views expressed in a part of the text. The intended outcome of this process is to assess the author attitude toward a particular topic, movie, product, etc. The result is positive, negative, or neutral. These study illustrated different techniques in SA approach for extracting and analytics sentiments associated with the polarity of positive, negative, or neutral on the topic selected. Social networks SA can be a useful source of information and data. SA acquires important in many areas of business, politics, and thought. So, this study contains a comprehensive overview of the most important studies in this ﬁeld from the past to the recent studies till 2017. The main aim of this study is to provide full concept about SA techniques and its classiﬁcation and methods used it. Also, we give a brief overview of big data techniques and its relation and use in SA ﬁeld. Because the recent period has witnessed a remarkable development in the use of Big Data (Hadoop) in the process collection of data and reviews from social networks for analysis. Keywords Big data · Classiﬁcation · Challenges · Sentiment analysis · Social media · Twitter 1 Introduction In present days, most of the people are expressing their feelings, opinions, and shar- ing their experiences, using the Internet and the social networks. This usually leads to communicate massive amount of data using the Internet. But most of these data are useful when analyzed; for example, most industrial companies and election cam- A. A. Q. Aqlan (B) · B. Manjula Department of Computer Science, Kakatiya University, Warangal 506009, Telangana, India e-mail: ameenaqlan218@gmail.com R. Lakshman Naik Department of Information Technology, Kakatiya University, Warangal 506009, Telangana, India © Springer Nature Singapore Pte Ltd. 2019 N. Chaki et al. (eds.), Proceedings of International Conference on Computational Intelligence and Data Engineering, Lecture Notes on Data Engineering and Communications Technologies 28, https://doi.org/10.1007/978-981-13-6459-4_16 147\n",
      "----------------------------------\n",
      "Paragraph 3: 148 A. A. Q. Aqlan et al. Fig. 1 Sentiment analysis process steps paigns rely on knowing the opinions of people through communication sites and see whether they are positive, negative, or neutral. The SA has emerged because of the huge information exchange on the Internet. The SA idea was ﬁrst proposed by Nasukawa [1]. Firstly, the SA is used for natural language process (NLP) [2], which analyzes opinions, feelings, reactions of people and writers on the Internet through social networking sites and business sites about the many products and services. Sentiment analysis is a broad ﬁeld for many researchers and can also be called opin- ion mining; because it helps to classify ideas and opinions as positive, negative, or neutral. SA is a textual study, which is widely used on reviews and surveys in the Internet and social media. It handles responses and customer feedback on commer- cial sites to know their acceptance or rejection of a product; this helps to improve the sales of the company as it tells the choice of a customer. With the explosion of different opinions through social networking sites, new ideas were generated by sys- tems, politicians, psychologists, manufacturers, and researchers to analyze them to implement the best decisions. Sentiment analysis has a high efﬁciency using NLP, as statistics, and machine learning approaches to extract and deﬁne sentiment content in a text unit. 2 Sentiment Analysis Sentiment analysis is becoming very important to study growing opinions faster and faster within social media and other sites, The huge explosion in information in recent years in the sites of communication, air trafﬁc and alternative markets, all this huge amount of information cannot be controlled and analyzed used the traditional way, so the scientists and researchers developed a high-efﬁciency techniques to deal with this data. This requires the SA to process data and know its polarity to determine the right decision. SA involves ﬁve steps to process data; those are data collection, text preparation, sentiment detection, sentiment classiﬁcation, and presentation of output [3] as shown in bellow (Fig. 1).\n",
      "----------------------------------\n",
      "Paragraph 4: A Study of Sentiment Analysis … 149 2.1 Data Collection The data collection is the ﬁrst step in sentiment analysis. The collection of data from sources like user groups, Twitter, Facebook, blogs and commercial website such as amazon.com and alibaba.com, etc. This data cannot be analyzed using traditional methods like scanning, text analysis, and language processing, which is used for extraction and classiﬁcation. Wei Xu [4] and Tapan Kumar [5] proposed a certain method for a task of paraphrasing and gathering the tweet data called Twitter Stream- ing API. 2.2 Text Preparation Text preparation examines the data before analyzing it. Some reviews and conversa- tions in the communication sites contain offensive and inappropriate words, so they are examined and preparation to be the result more reliable analysis. This process selects the contents that are not related to the analysis and then removes it. Objec- tive of the process is the removal of spam and inappropriate reviews before sent to automated analysis. In this case, we can use part of speech (POS) technique which are used for text preparation before analysis [6, 7]. 2.3 Sentiment Detection Sentiment detection is the process of ﬁnding the sentiment newline expressed in a review by using machine learning technique or NLP technique; these are also called opinion mining (OM) new line and sentiment analysis. Sentiment detection consists of the examination of phrases and sentences extracted from reviews and ideas. All the sentences containing self-expressions like beliefs, opinions, and abuse are retained. Many research studies in this ﬁeld included different methods of detection, like Lakshmish Kaushik [8]; one of the recent studies propose a system for automatic sentiment detection in natural audio streams by using POS technique. 2.4 Sentiment Classiﬁcation Sentiment Classiﬁcation is a task to extraction and classiﬁcation the text whose objective to classify according to a polarity of the opinion it contains (pang 2002), e.g. positive or negative, good or bad, like or dislike. Sentiment classiﬁcation contains multiple techniques, and it is classiﬁed into three main techniques, namely machine learning approach, hybrid techniques approach, and lexicon-based approach [9] and\n",
      "----------------------------------\n",
      "Paragraph 5: 150 A. A. Q. Aqlan et al. Fig. 2 Sentiment classiﬁcation techniques [10]. Presently, Naive Bayes technique and support vector machines (SVMs) are more popular and used for sentiment classiﬁcation. These techniques can improve an accuracy of classiﬁcation of Tweets, such as Ankur Goel [11] use Naive Bayes for Sentiment Analysis Tweets in high speed. Therefore, sentiment analysis has a great deal of research in this ﬁeld and found that many applications and improvements have occurred in sentiment analysis in recent years. In this study, we will clariﬁcation most of the research in this area. These articles cover most of the divisions and classiﬁcations widely in SA ﬁelds. Sentiment classiﬁcation techniques are discussed with more emphasis on most details and related points and originating references. In Fig. 2, we will illustrate all techniques which are currently used in sentiment classiﬁcation until 2017. 2.4.1 Lexicon-Based Approach Multiple words are used to classify sentiment and use positive words for the desired things, while using negative words for undesired things. So, lexicon-based approach relies mainly on ﬁnding opinion lexicon, which is used for text analysis. There are two methods according to lexicon-based approach. The ﬁrst one is corpus-based approach, and the second one is dictionary-based approach. Corpus-Based Approach: The corpus-based approach starts with a seed list of opinion words and then ﬁnds other ideas from the words in a large corpus to get opinions from certain directions. In another meaning, most methods rely on gram- matical patterns or that occur together with the seed list of opinion words to ﬁnd other\n",
      "----------------------------------\n",
      "Paragraph 6: A Study of Sentiment Analysis … 151 words from a large corpus. Hatzivasiloglu [12] is one of the most important methods to represent the corpus-based approach. The ﬁrst step to start was to create a seed list and use it with a wide range of language restrictions to be able to identify additional words including their orientations. To implement corpus-based approach, we use two different approaches: statistical approach and semantic approach as illustrated in the following. i. Statistical approach: It is used in many applications that have a relation in the ﬁeld of SA. The famous of them is the one that can detect the manipulation of the review by conducting a statistical test of randomization which is called runs test [13]. ii. Semantic approach: It gives values to sentiments while relies on more than principle to calculate the afﬁnity and similarity of different words. The basis of this principle is to support the Sentiment value in the words and words close. WorldNet [14]. Dictionary-Based Approach: Dictionary-based approach presented a compre- hensive strategy for the dictionary-based approach. In this famous strategy, a small group of words is hand-picked with known trends [15, 16]. Then, we come to plant this group of words by searching in the known approach corpora thesaurus [17] or WorldNet [18] for all synonyms and antonyms. The new words that are found are added to the seed list, and then, the next repetition begins. This repetitive process continues and stops only when there are no new words. 2.4.2 Machine Learning Approach Machine learning approach is used to solve the problems related to text classiﬁcation that contain syntactic or linguistic features. Whilst lexicon-based approach is used to extracting sentiment from text, it depends on a sentiment lexicon; the collection of known and pre-compiled sentiment terms in Machine Learning algorithms divided intoReinforcementLearning[17],UnsupervisedLearning,andSupervisedLearning. Reinforcement Learning Technique: Its entirety indicates how to make an opti- mal decision an important technique that differs relatively from its counterpart unsu- pervised learning. This technique is highly concerned with improving the efﬁciency of text classiﬁcation to show that the reinforcement learning technique is important and prominent. Unsupervised Approach: It is a unique type of machine learning algorithm and is used in most cases to draw and diverse inferences of data; these groups of data consist of input data without any labeled responses. It is used when it is impossible to obtain labeled training documents. Supervised Learning: It is a type of machine learning approach that uses a data set called training data set to make predictions. These data set contain input data as well as response values. In supervised learning methods, it makes use of a large number of assorted training documents.\n",
      "----------------------------------\n",
      "Paragraph 7: 152 A. A. Q. Aqlan et al. Probabilistic Classiﬁers: It uses many models for classiﬁcation. There are several types of mixture models; each mixture model must be an integrated mixture com- ponent. Each type of this mixture acts as generative and can support the particular taking term for this component or other; this approach is called generative classiﬁer. i. Maximum Entropy Classiﬁer: The maximum entropy classiﬁer is a kind of clas- siﬁcations normally used in NLP, speech, data, and addressing problems. Max- imum entropy is also probability distribution estimation; it is an important and famous technique widely used for a variety of natural language tasks, such as language modeling, part-of-speech tagging, and text segmentation. The under- lying principle of maximum entropy is without external knowledge. ii. Bayesian Network classiﬁer: The most important assumption of the Bayesian network classiﬁer is a set of variables, each variable containing a limited set of mutual cases. It is independent of the features. The real assumption is a suggestion intended to have all the features which are completely dependent. This certainly leads to a certain model of Bayesian network which is a guided graph and represents a random contract. iii. Naïve Bayes: Naïve Bayes is the most popular method for text classiﬁcation recently. Naïve Bayes classiﬁer model computes the back probability of the class, based on the division of words in the adopted document. Rule-based Classiﬁcation: Rule-based classiﬁcation is used for any scheme that constructs the classiﬁcation according to rules IF and THEN. Linear classiﬁer is a vector of real-valued numerical input features. Linear classiﬁer: is a decision based on the value of a linear combination of characteristics. An object’s characteristics are also known as feature values and are typically presented to the machine in a vector called a feature vector; linear classiﬁer is divided into two methods; those are: i. Neural Network: Neural network is a continuum of algorithms based on the recognition of the relationships inherent in several sets of data using a process similar to the way the human mind works. ii. Support vector machine: SVM is used to analyze datasets for classiﬁcation and regression analysis, it is a machine learning algorithm that leads to process data automatically. Decision Tree Classiﬁers (DTC): These are used for classiﬁcation. It’s aims to divide large data into small groups to be easy control, the DTC use multi-values of attributes, and features of the data to appear a class label discrete prediction. It is a fairly simple technique and widely used in sentiment analysis ﬁeld. 2.4.3 Hybrid Techniques Approach Hybrid techniques approach is a combination of multiple computational techniques which provide greater advantages than individual techniques and improve sentiment\n",
      "----------------------------------\n",
      "Paragraph 8: A Study of Sentiment Analysis … 153 (data) analysis. Use of this technique is very convenient for many because it combines two or more technologies, so it shows much better results than other methods. 2.5 Presentation of Output The main objective of analyzing a huge amount of data is to convert unstructured text into useful information and then to display it through charts such as a graph, line graph, and bar graph. 3 Background SA is contextual mining of texts; it identiﬁes the sentences and subjective information to classify opinions according to polarity. Sentiment analysis studies people’s feel- ings, opinions, assessments, and attitudes toward many services, issues, events, and organizations [17]. SA is not only applied to the commercial product reviews; it can also be applied to all types of social communication sites and stock markets. Three topics work under the umbrella of sentiment analysis emotions detection, building resources, and transfer learning. Emotions detection is a recent ﬁeld of research that is closely related to SA. The aim of SA is to detect positive, negative, or neutral, feelings from the text, whereas emotion detection aims to detect and recognize types of feelings through the expression of texts, such as anger, disgust, fear, happiness, sadness, and surprise. Building resources is a lexicon; it is a vocabulary that is used to express an opinion according to the polarity either positive, negative, or neutral. Transfer learning is considered as the transfer of knowledge from one learned task to a new task in machine learning. The text classiﬁcation according to the following criteria is as follows: The ﬁrst standard is the polarity of sentiment into (positive, negative, or neutral). The second standard is the polarity of the outcome that applies to most political articles and medical facilities for managing disease data as follows [19]: • Use agree or disagree, e.g., political debates [20]; • Criteria good or bad [21]; • Pros and cons: The meaning of this is either positive or negative [22]; in the following ﬁgure, will clarify steps of Sentiment Classiﬁcation and related works in SA.\n",
      "----------------------------------\n",
      "Paragraph 9: 154 A. A. Q. Aqlan et al. Fig. 3 Knowledge discovery and pattern recognition architecture 3.1 Knowledge Discovery and Pattern Recognition Architecture Social networking sites are many and full of useful data; however, there are important and credible data and useless data (not useful). The reliability data is usually found in cultural sites or shopping sites because the customer may have experience in dealing with commercial sites. So, we have illustrated the kinds of techniques and classiﬁca- tions within the ﬁeld of sentiment analysis and how to extract and manipulate data to reach reliable results. This section will illustrate a knowledge discovery and pattern recognition architecture, Fig. 3. 3.1.1 Social Network The Internet is the right environment and the main source for most information and ideas that are shared by users. It is a resourceful place with respect to sentiment information. By following a lot of ideas and articles, we found that people pre- fer to publish their content through various online social media, such as forums, microblogs, or online social networking sites. Choosing data source is the ﬁrst step in SA. All communication site a fertile environment for data collection; most of\n",
      "----------------------------------\n",
      "Paragraph 10: A Study of Sentiment Analysis … 155 the data is useful, and often, there is abusive data which not useful, these data are excluded automatically during analysis. 3.1.2 Data Collection Application program interface (API)—this is the proposed system to extract the data and download. It is characterized by research for hashtags, main keywords, and other classiﬁers simultaneously [23]. API is widely used to collect reviews by researchers and interested companies, but now we can use different Application program to collect data such as Hadoop ﬂume, Shirahatti [21]. 3.1.3 Natural Language Processing Natural Language Processing: It deals with all human languages whether written or oral to process and apply. This is the main propose of NLP. NLP refers to part of text; part of text includes verbs, adjectives, and nouns. We may get unorganized data in this case and need further processing using one of the algorithms (part of speech or N gram). Part of Speech can divide the sentences to small words, and each word has a meaning; for example, in English language each small word can have a distinctive meaning and this comes according to its use and functions, and is categorized into several types or small parts of speech such as noun, pronoun, verb, adverb, adjective, conjunction, preposition, and interjection. N Gram, An n gram is simply a sequence of tokens. In the context of computa- tional linguistics, these tokens are usually words, though they can be characters or subsets of characters. The n simply refers to the number of tokens. n gram is used for word sequence itself or predictive model that assigns it a probability. The gram is a combination of letters; n gram refers to divide the sentence into several parts (count the word); for example, “Friday will be holiday”—this is contains 4 gram; “tomor- row is holiday”—this is contains 3 gram. Named-entity recognition (NER), using to divides comments or the tweet into smaller parts this parts each one containing two words. 3.1.4 Preprocessing Preprocessing relies mainly on ﬁnding opinion lexicon, which is used for text anal- ysis; it classiﬁes the words either positive, negative, or neutral. 3.1.5 Feature Identiﬁer Feature identiﬁer: It is to identify and classify the entities being referred to as tweets. Final stage SVM classiﬁer is trained in order to obtain the tweet’s label.\n",
      "----------------------------------\n",
      "Paragraph 11: 156 A. A. Q. Aqlan et al. 3.1.6 Classiﬁer Approach Classiﬁer Approach: It is used to analyze datasets for classiﬁcation and regression analysis. It classiﬁes the opinion positive, negative, or neutral in a ﬁnal stage. 3.1.7 Result Analyzation Result Analyzation or sentiment analysis commonly uses several ratings to express the abundance of feelings or versa. Sentiment is evaluated through the use of stars; some Websites require the evaluation of their material by using stars, and we will illustrate the rating of stars commonly used as follows [24] and [25]. • Positive +2 or 5 stars • Rational positive +1 or 4 stars • Neutral 0 or 3 stars • Rational negative –1 or 2 stars • Negative –2 or 1 star 4 Related Work The purpose of this study is to give a clear conception of most techniques in the ﬁeld of sentiment analysis, where it is easier for new researchers to beneﬁt from it. As mentioned many techniques of analysis, we will clarify some studies and research in recent years that dealt with this area; this paper also covers a wide ﬁeld of sentiment classiﬁcation technique and approach in SA ﬁeld. Lexicon-based technique aims to extract and collect data from social network such as Twitter [25], Facebook, etc. by use API Graph to collect and load all the target data for analysis, and examine all words that do not represent an emotional value or feature, then created a list of words and analyzed them, that would be used in all cases, these shown positive results in predict the sentiment behind a status post on Facebook by use lexicon-based approach with high efﬁciency. Machine learning approach is not limited to the analysis of data in social media, Where used to know the driver’s sense at the moment of leadership. One of them sought to generate and know the rules of the cognitive deviations of the drivers directly from the place of the driving simulation environment. Through this study, the eye movements of the drivers were taken using a simulated device [26]. Dictionary-based approach is used with high efﬁciency in the ﬁeld of SA. Seongik Park build thesaurus lexicon characterized in clearly and credibility [27], Where build this approach through three online dictionaries to gathering thesauruses based on the seed words, and sought to stores the real words which can be trusted into the thesaurus lexicon in order to improve the reputation and credibility of the thesaurus lexicon, and prove it a prominent lexicon.\n",
      "----------------------------------\n",
      "Paragraph 12: A Study of Sentiment Analysis … 157 Use the DBA to build thesaurus lexicon. The purpose of this is to increase the availability of tweet and review for the sentiment classiﬁcation without the need to use human resource. However, accuracy obtained was slightly increased. Ishtiaq Ahsan has build a methodology for reviewing opinions and detecting spam through a well-known learning method (active learning and supervision) with the use of all data including real and fabricated show us very promising results while conducting several different experiments [21]. The results have shown that detection method is very effective and promising. The use of this technique is very convenient for many because it combines two or more technologies, so it shows much better results than other methods. Seongik Park [21] uses the dictionary-based approach to build a lexicon for sen- timent classiﬁcation and uses three online dictionaries rich in vocabulary to collect thesauruses based on the seed words to improved reliability of the lexicon. The- sauruses are a collection of antonyms and synonyms to expand the lexicon more vocabulary. Because he focused only on lexicon building, the result was slightly increased. 5 Big Data Massive amounts of data are stored in communication sites. These data are increasing rapidly every year. Most of the data stored on the Internet has been produced in recent years. But most of the data produced may be useful when processed. Business, political, and social sites are the main source of data growth and the largest data incubator. Big data can be deﬁned as a collection and storage of huge amounts of information for ﬁnal processing; there is a concept that gained momentum among the public in the early 2000s almost by Doug Laney which explain the current mainstream deﬁnition of big data. Big data is of high volume, high velocity, and high variety. Highvolumemeanslargeamountsofinformationarecollectedfromdifferentsources such as commerce, social media, and other communication channels and then stored in Hadoop support. High velocity is a high capability of retrieval data, sharing into slaves, process speed should be very high, high variety means all varieties of data processing, such as structured, unstructured, and semi structured. Big data involves various tools including Hadoop, data science, MongoDB, data mining, Teradata, and Python. Hadoop is a big data framework to store and process high volumes of data with very high speed. Hadoop is an open source. Hadoop satisﬁesallcharacteristicsofbigdata.SoyoucansayHadoopasabigdataframework. Hadoop framework includes two main modules: Hadoop MapReduce and Hadoop Distributed File System. • MapReduce is a processing technique and a program model for distributed com- puting based on Java. MapReduce algorithm involves two important tasks those are map and reduce. The map is A set of data which is converted to another set of\n",
      "----------------------------------\n",
      "Paragraph 13: 158 A. A. Q. Aqlan et al. Fig. 4 Data science terms data (map is input value) Reduce job takes the output from a map; reduce is output of data value, the reduce task is always performed after the map job. • Hadoop Distributed File System (HDFS) is designed to store a huge data sites reliably. HDFS creates several replicas of a huge data for reliability and then put them on compute nodes around the cluster. Data Science: It is very suitable for data analysis, so it can give a boost of improvements to sentiment analysis. Data science is dealing with each structured data, unstructured data, and semi-structured data. In simple terms, it incorporates statistics, mathematical, analysis, signal processing, natural language processing, etc. Data science is an umbrella which involves several terms as illustrated in Fig. 4. 5.1 Big Data Tools in Sentiment Analysis Using the modern program in big data is very important for sentiment analysis at the moment. The use of big data for sentiment is very appropriate and still at the begin- ning of growth. Most of the comments available in social media are unstructured. So, we can use Hadoop technology because Hadoop can deal with structured data, unstructured data, and semi-structured data. A research study is conducted to deter- mine diabetes awareness among different segments of the population using Hadoop MapReduce, [21]. The author Monu Kumar [28] extracts and collect the data from social networks and analyze it by using big data technique. Hadoop MapReduce is processing technique of big data effectively, so priorities use Hadoop for data col-\n",
      "----------------------------------\n",
      "Paragraph 14: A Study of Sentiment Analysis … 159 lection and analysis. Shirahatti [21] collects comments and reviews from Twitter using streaming tool ﬂume then processes it in Hadoop MapReduce. The result of processing time taken was very less compared to others previous methods. 6 Sentiment Analysis Challenges The researcher in this ﬁeld faces several of constraints and challenges which come in the form of sentences or words vague difﬁcult to identify. These constraints constitute an obstacle to analyze targeted data and may lead to an unreliable outcome. There are different types of these obstacles that pose challenging for sentiment analysis using one of the known questionnaires, simple questionnaires, or role-based questionnaires [22]. But the clear events is highly accepted and a candidate to obtaining good comments and high-quality. We will illustrate some types of challenges which are listed below. i. Success or fail from one side:In many events that occur daily, expressions and phrases are often used to describe the win or loss of one side. For example, Brazil defeated Argentina 2-1; The Supreme Court ruled in favor of early marriage, the regime’s army suppressed a popular uprising. In this case, if the person supports Brazil,earlymarriage,andtherulingregime,willconsidertheseeventsaspositive. but the person supporting Argentina and marriage over the age of eighteen, and the popular uprising will consider these events as negative. Also, we note that the transfer of the proceedings of the event as the win of one party or (as the loss of another party) does not mean that the speaker expresses a negative or positive opinion towards one of the parties mentioned. For example, when Sevilla defeated Real Madrid 2-1 in the Spanish League 2017, the news around the world was reported approximately that Real Madrid lost from Sevilla, instead of broadcasting the news Sevilla beat on Real Madrid. This does not refer that speakers or those involved in the broadcast reports expressed a negative opinion on the Real Madrid team, but the royal team was their focus and they thought that Real Madrid would not easily lose and owns with him forty matches without losing pre-Sevilla match. ii. Precisely understanding what is meant by opinion: Sometimes, it is difﬁcult for a person to understand the exact meaning of some sentences. For example, glad to expose the disciplinary committee steroids of player Noor. From this context, the target of the view seems unclear, do he mean Noor, Noor doping, or Noor doping that which are revealed. In this case, the person can conclude that the news carrier has a negative idea toward Noor doping and probably Noor in general. iii. Neutral reports of events: When the announcer starts broadcasting the reports, the speaker must give a signal to his or her emotional state before describing the events or situations, because It is unclear whether these reports should be\n",
      "----------------------------------\n",
      "Paragraph 15: 160 A. A. Q. Aqlan et al. considered as emotional about evolution or presupposes whether the speaker is in a negative emotional state (Happy, angry, cheerful, sad,). iv. Detection abusive opinions and counterfeit reviews: On the Web, there is a lot of spam information which contains spam and abusive reviews for sentiment classiﬁcation; it is unacceptable to process data with a presence of fake data because this reduces the reliability of the results; we should initially identify unwanted messages and remove them and then the processing, These steps we can do through reviewer [29]. v. The focus on one domain: This challenge is a major obstacle to sentiment analysis because it depends mainly on a limited nature from sentiment analysis word; this may lead to focus on only one topic. For example, we may ﬁnd in one domain several features and good performance; at the same time, these features may be very bad in some other domains [30]. vi. Difﬁcult acquisition of opinion mining software: The software of opinion mining is very expensive, and their prices are high and can only be bought by governments and large organizations for now. These high prices exceed people’s expectations and will remain a major obstacle to people wanting to get this software; the average person cannot buy this expensive software. Therefore, this software should be available to all categories of society without exception so that everyone beneﬁts from them. 7 Conclusion This study mainly focuses on the overview of different techniques used in the ﬁeld of SA. Among thirty-eight papers recently published till 2017, we discussed the impor- tance of opinions and comments on Web sites and how to extract them through cer- tain techniques. We have noted important techniques in this area including the most famous as Naive Bayes and SVM are the more commonly utilized in machine learn- ing algorithms, to solve sentiment classiﬁcation problem. Many present researchers have improved the scope of this ﬁeld. The aim of this study gives overview on these improvements and summarizes categories of articles given according to different sentiment analyses. The contribution of this study takes a brief look at the use of big data (Hadoop tools) in sentiment analysis ﬁeld. Use a big data is a fairly new study in SA ﬁeld. Therefore, our future work will focus on using big data techniques (Hadoop) in sentiment analysis ﬁeld to give more effective and accurate results. This paper will be useful for new researchers and who has a desire to join this ﬁeld. In this study, we covered all the techniques and the most famous in one paper and illus- trated different techniques in (SA) approach for extracting and analytics sentiments associated with the polarity of positive or negative, or neutral on the topic selected.\n",
      "----------------------------------\n",
      "Paragraph 16: A Study of Sentiment Analysis … 161 References 1. Nasukawa Y (2003) Sentiment analysis: capturing favorability using natural language process- ing, IBM Almaden Research Center, CA 95120, https://doi.org/10.1145/945645.945658 2. Mohey D (2016) A survey on sentiment analysis challenges. J King Saud Univ Eng https://doi. org/10.1016/j.jksues.2016.04.002 3. Alessia D (2015) Approaches, tools and applications for sentiment analysis implementation. Int J Comput Appl 125(3) 4. Xu W , Ritter A, Grishman R (2013) Gathering and generating paraphrases from twitter with application to normalization 5. Hazra TK (2015) Mitigating the adversities of social media through real time tweet extraction system, IEEE, https://doi.org/10.1109/iemcon.2015.7344483 6. Semih Y (2014) Tagging accuracy analysis on part-of-speech taggers. J Comput Commun 2:157–162, https://doi.org/10.4236/jcc.2014.24021 7. El-Din DM (2015) Online paper review analysis. Int J Adv Comput Sci Appl 6(9) 8. Kaushik L (2013) Sentiment extraction from natural audio streams, IEEE https://doi.org/10. 1109/icassp.2013.6639321 9. Vaghela VB (2016) Analysis of various sentiment classiﬁcation techniques. Int J Comput Appl 140(3) 10. BiltawiL M (2016) Sentiment classiﬁcation techniques for Arabic language a survey, IEEE, https://doi.org/10.1109/iacs.2016.7476075 11. Goel A (2016) Real time sentiment analysis of tweets using naive bayes, IEEE, https://doi.org/ 10.1109/ngct.2016.7877424 12. Hu M, Liu B (2004) Mining and summarizing customer reviews, seattle, Washington, USA, https://doi.org/10.1145/1014052.1014073 13. Kim S-M (2004) Determining the sentiment of opinions, ACM Digital Library, https://doi.org/ 10.3115/1220355.1220555 14. Mohammad S (2009) Generating high-coverage semantic orientation lexicons from overtly marked words and a thesaurus. In: Conference on empirical methods in natural language pro- cessing, pp 599–608 15. Miller GA (1993) Introduction to word net: an on-line lexical database 16. Hatzivassiloglou V, McKeown R (1998) Predicting the semantic orientation of adjectives, New York, N.Y.10027, USA 17. Medhat W (2014) Sentiment analysis algorithms and applications a survey. Ain Shams Eng J (Elsevier B.V.), 5(4):1093–1113 18. Soo-Min Kim, Determining the Sentiment of Opinions, International Journal, doi=10.1.1.68.1034, (2004) 19. Pang B, Lee L (2008) Opinion mining and sentiment analysis. https://doi.org/10.1561/ 1500000011 20. Niu Y (2005) Analysis of polarity information in medical text, PMC Jurnal 21. Park S (2016) Building thesaurus lexicon using dictionary based approach for sentiment clas- siﬁcation, IEEE, https://doi.org/10.1109/sera.2016.7516126 22. Ramsingh J (2016) Data analytic on diabetic awareness with Hadoop streaming using map reduce in Python, IEEE, https://doi.org/10.1109/icaca.2016.7887979 23. Kim S-M, Hovy E (2006) Automatic identiﬁcation of pro and con reasons in online reviews, ACM Digital Library 24. Trupthi M (2017) Sentiment analysis on twitter using streaming API, IEEE, https://doi.org/10. 1109/iacc.2017.0186 25. Cambria E, Hussain A (2015) Group Using Lexicon Based Approach. Springer J https://doi. org/10.1007/978-3-319-23654-4 26. Akter S (2016) Sentiment analysis on Facebook group using lexicon based approach, IEEE, https://doi.org/10.1109/ceeict.2016.7873080 27. Yoshizawa A (2016) Machine-learning approach to analysis of driving simulation data, IEEE, https://doi.org/10.1109/icci-cc.2016.7862067\n",
      "----------------------------------\n",
      "Paragraph 17: 162 A. A. Q. Aqlan et al. 28. Istiaq Ahsan MN (2016) An ensemble approach to detect review spam using hybrid machine learning technique, IEEE, https://doi.org/10.1109/iccitechn.2016.7860229 29. Kumar M (2016) Analyzing Twitter sentiments through big data, IEEE, https://doi.org/10. 1109/sysmart.2016.7894530 30. Abhinandan P, Shirahatti (2015) Sentiment analysis on Twitter data using Hadoop. Int J Eng Res Gen Sci 3(6) View publication stats\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import fitz \n",
    "pdf_path = './AstudyofSentimentanalysis.pdf'\n",
    "paragraphs = []\n",
    "pdf_document = fitz.open(pdf_path)\n",
    "#iterate through pages\n",
    "for page_number in range(pdf_document.page_count):\n",
    "    page = pdf_document[page_number]\n",
    "    page_text = page.get_text(\"text\")\n",
    "    #split text into lines\n",
    "    lines = [line.strip() for line in page_text.split('\\n')]\n",
    "    print(len(lines))\n",
    "    #concatenate lines into paragraphs\n",
    "    current_paragraph = \"\"\n",
    "    for line in lines:\n",
    "        if line:\n",
    "            current_paragraph += line + \" \"\n",
    "        else: \n",
    "            if current_paragraph:\n",
    "                paragraphs.append(current_paragraph.strip())\n",
    "                current_paragraph = \"\"\n",
    "    #adding the last paragraph if not empty\n",
    "    if current_paragraph:\n",
    "        paragraphs.append(current_paragraph.strip())\n",
    "\n",
    "pdf_document.close()\n",
    "result_paragraphs = paragraphs\n",
    "\n",
    "#viewing paragraphs selected\n",
    "print (f\"there are \")\n",
    "for i, paragraph in enumerate(result_paragraphs, start=1):\n",
    "    print(f\"Paragraph {i}: {paragraph}\")\n",
    "    print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9387ad",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3> <ol>2. similarity measure</ol></h3>\n",
    "\n",
    "<b> Now it is time for paragraph extraction based on similarity to a specified topic context-based.\n",
    "In this stage I had three approaches (simple, lazy, model-based) :\n",
    "<ol> \n",
    "    <ol> a) simple: key word matching </ol> \n",
    "    <ol> b) lazy: using NLP library like Spacy </ol> \n",
    "    <ol> c) cosine similarity:  using pre-built model for feature extraction </ol> \n",
    "</ol>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cb11c7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3> <ol> a) Simple approach: keyword matching </ol></h3>\n",
    "It is the most simple way by checking if that the topic sentence can be found inside the paragraph.\n",
    "This is a simple way that has some drawbacks: - \n",
    "<ol> \n",
    "    <li>  it is not context based similarity </li> \n",
    "    <li>  it depends on the presence of the words inside a paragraph </li> \n",
    "</ol>\n",
    "for example the research I am using is a study for sentiment analysis, so It is logical that data analysis topic is being discussed at most of the paragraphs, but It extracts 1 paragraph only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "ca654ab2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#number of extracted paragraphs:  1\n",
      "-----------------------------\n",
      "158 A. A. Q. Aqlan et al. Fig. 4 Data science terms data (map is input value) Reduce job takes the output from a map; reduce is output of data value, the reduce task is always performed after the map job. • Hadoop Distributed File System (HDFS) is designed to store a huge data sites reliably. HDFS creates several replicas of a huge data for reliability and then put them on compute nodes around the cluster. Data Science: It is very suitable for data analysis, so it can give a boost of improvements to sentiment analysis. Data science is dealing with each structured data, unstructured data, and semi-structured data. In simple terms, it incorporates statistics, mathematical, analysis, signal processing, natural language processing, etc. Data science is an umbrella which involves several terms as illustrated in Fig. 4. 5.1 Big Data Tools in Sentiment Analysis Using the modern program in big data is very important for sentiment analysis at the moment. The use of big data for sentiment is very appropriate and still at the begin- ning of growth. Most of the comments available in social media are unstructured. So, we can use Hadoop technology because Hadoop can deal with structured data, unstructured data, and semi-structured data. A research study is conducted to deter- mine diabetes awareness among different segments of the population using Hadoop MapReduce, [21]. The author Monu Kumar [28] extracts and collect the data from social networks and analyze it by using big data technique. Hadoop MapReduce is processing technique of big data effectively, so priorities use Hadoop for data col-\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "#simple keyword matching\n",
    "topic = \"data analysis\"\n",
    "relevant_paragraphs_keyword_matching = []\n",
    "for paragraph in result_paragraphs:\n",
    "    if topic.lower() in paragraph.lower():\n",
    "        relevant_paragraphs_keyword_matching.append(paragraph)\n",
    "print(\"#number of extracted paragraphs: \", len(relevant_paragraphs_keyword_matching))     \n",
    "print (\"-----------------------------\")\n",
    "for paragraph in relevant_paragraphs_keyword_matching:\n",
    "    print (paragraph)\n",
    "    print (\"-----------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0c707c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3> <ol> b) Lazy approach: NLP library (Spacy) </ol></h3>\n",
    "In this approach I am using a NLP library spacy that has ready methods for comparing similaries between two texts depending on some factors, for example, searching for the same topic (data analysis), now it extracts 7 paragraphs (depending on the threshold I am providing that can be changed to add more or less paragraph),\n",
    "<b> but again it has some drawbacks like : - </b>\n",
    "<ol> \n",
    "    <li>  no customization available </li> \n",
    "    <li>  no control on similarity measurement way </li> \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "e7d094f9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (3.7.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (8.2.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (63.4.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.5)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.14)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: colorama in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-md==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
      "     -------                                  7.9/42.8 MB 59.8 kB/s eta 0:09:44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 435, in _error_catcher\n",
      "    yield\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 516, in read\n",
      "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 90, in read\n",
      "    data = self.__fp.read(amt)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\http\\client.py\", line 463, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\http\\client.py\", line 507, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\ssl.py\", line 1242, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\ssl.py\", line 1100, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 167, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 247, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 369, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 73, in resolve\n",
      "    collected = self.factory.collect_root_requirements(root_reqs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 491, in collect_root_requirements\n",
      "    req = self._make_requirement_from_install_req(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 453, in _make_requirement_from_install_req\n",
      "    cand = self._make_candidate_from_link(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 206, in _make_candidate_from_link\n",
      "    self._link_candidate_cache[link] = LinkCandidate(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 297, in __init__\n",
      "    super().__init__(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 162, in __init__\n",
      "    self.dist = self._prepare()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 231, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 308, in _prepare_distribution\n",
      "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 438, in prepare_linked_requirement\n",
      "    return self._prepare_linked_requirement(req, parallel_builds)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 483, in _prepare_linked_requirement\n",
      "    local_file = unpack_url(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 165, in unpack_url\n",
      "    file = get_http_url(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 106, in get_http_url\n",
      "    from_path, content_type = download(link, temp_dir.path)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\network\\download.py\", line 147, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\progress_bars.py\", line 53, in _rich_progress_bar\n",
      "    for chunk in iterable:\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 63, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 573, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 538, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\contextlib.py\", line 137, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 440, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='objects.githubusercontent.com', port=443): Read timed out.\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_md  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "cec0dd18",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5209841097760008\n",
      "0.6080865312919047\n",
      "0.6053160480948162\n",
      "0.6334433351859559\n",
      "0.5850021816165465\n",
      "0.6049219303564474\n",
      "0.5975953137306559\n",
      "0.5892066464037059\n",
      "0.644247049879054\n",
      "0.5989821345397016\n",
      "0.5952615417315948\n",
      "0.6402676662677048\n",
      "0.7246099085517221\n",
      "0.524753623516531\n",
      "0.5995859082951902\n",
      "0.48143347382112084\n",
      "0.5324107560264297\n",
      "#number of extracted paragraphs:  7\n",
      "-----------------------------\n",
      "A Study of Sentiment Analysis: Concepts, Techniques, and Challenges Ameen Abdullah Qaid Aqlan, B. Manjula and R. Lakshman Naik Abstract Sentiment analysis (SA) is a process of extensive exploration of data stored on the Web to identify and categorize the views expressed in a part of the text. The intended outcome of this process is to assess the author attitude toward a particular topic, movie, product, etc. The result is positive, negative, or neutral. These study illustrated different techniques in SA approach for extracting and analytics sentiments associated with the polarity of positive, negative, or neutral on the topic selected. Social networks SA can be a useful source of information and data. SA acquires important in many areas of business, politics, and thought. So, this study contains a comprehensive overview of the most important studies in this ﬁeld from the past to the recent studies till 2017. The main aim of this study is to provide full concept about SA techniques and its classiﬁcation and methods used it. Also, we give a brief overview of big data techniques and its relation and use in SA ﬁeld. Because the recent period has witnessed a remarkable development in the use of Big Data (Hadoop) in the process collection of data and reviews from social networks for analysis. Keywords Big data · Classiﬁcation · Challenges · Sentiment analysis · Social media · Twitter 1 Introduction In present days, most of the people are expressing their feelings, opinions, and shar- ing their experiences, using the Internet and the social networks. This usually leads to communicate massive amount of data using the Internet. But most of these data are useful when analyzed; for example, most industrial companies and election cam- A. A. Q. Aqlan (B) · B. Manjula Department of Computer Science, Kakatiya University, Warangal 506009, Telangana, India e-mail: ameenaqlan218@gmail.com R. Lakshman Naik Department of Information Technology, Kakatiya University, Warangal 506009, Telangana, India © Springer Nature Singapore Pte Ltd. 2019 N. Chaki et al. (eds.), Proceedings of International Conference on Computational Intelligence and Data Engineering, Lecture Notes on Data Engineering and Communications Technologies 28, https://doi.org/10.1007/978-981-13-6459-4_16 147\n",
      "-----------------------------\n",
      "148 A. A. Q. Aqlan et al. Fig. 1 Sentiment analysis process steps paigns rely on knowing the opinions of people through communication sites and see whether they are positive, negative, or neutral. The SA has emerged because of the huge information exchange on the Internet. The SA idea was ﬁrst proposed by Nasukawa [1]. Firstly, the SA is used for natural language process (NLP) [2], which analyzes opinions, feelings, reactions of people and writers on the Internet through social networking sites and business sites about the many products and services. Sentiment analysis is a broad ﬁeld for many researchers and can also be called opin- ion mining; because it helps to classify ideas and opinions as positive, negative, or neutral. SA is a textual study, which is widely used on reviews and surveys in the Internet and social media. It handles responses and customer feedback on commer- cial sites to know their acceptance or rejection of a product; this helps to improve the sales of the company as it tells the choice of a customer. With the explosion of different opinions through social networking sites, new ideas were generated by sys- tems, politicians, psychologists, manufacturers, and researchers to analyze them to implement the best decisions. Sentiment analysis has a high efﬁciency using NLP, as statistics, and machine learning approaches to extract and deﬁne sentiment content in a text unit. 2 Sentiment Analysis Sentiment analysis is becoming very important to study growing opinions faster and faster within social media and other sites, The huge explosion in information in recent years in the sites of communication, air trafﬁc and alternative markets, all this huge amount of information cannot be controlled and analyzed used the traditional way, so the scientists and researchers developed a high-efﬁciency techniques to deal with this data. This requires the SA to process data and know its polarity to determine the right decision. SA involves ﬁve steps to process data; those are data collection, text preparation, sentiment detection, sentiment classiﬁcation, and presentation of output [3] as shown in bellow (Fig. 1).\n",
      "-----------------------------\n",
      "A Study of Sentiment Analysis … 149 2.1 Data Collection The data collection is the ﬁrst step in sentiment analysis. The collection of data from sources like user groups, Twitter, Facebook, blogs and commercial website such as amazon.com and alibaba.com, etc. This data cannot be analyzed using traditional methods like scanning, text analysis, and language processing, which is used for extraction and classiﬁcation. Wei Xu [4] and Tapan Kumar [5] proposed a certain method for a task of paraphrasing and gathering the tweet data called Twitter Stream- ing API. 2.2 Text Preparation Text preparation examines the data before analyzing it. Some reviews and conversa- tions in the communication sites contain offensive and inappropriate words, so they are examined and preparation to be the result more reliable analysis. This process selects the contents that are not related to the analysis and then removes it. Objec- tive of the process is the removal of spam and inappropriate reviews before sent to automated analysis. In this case, we can use part of speech (POS) technique which are used for text preparation before analysis [6, 7]. 2.3 Sentiment Detection Sentiment detection is the process of ﬁnding the sentiment newline expressed in a review by using machine learning technique or NLP technique; these are also called opinion mining (OM) new line and sentiment analysis. Sentiment detection consists of the examination of phrases and sentences extracted from reviews and ideas. All the sentences containing self-expressions like beliefs, opinions, and abuse are retained. Many research studies in this ﬁeld included different methods of detection, like Lakshmish Kaushik [8]; one of the recent studies propose a system for automatic sentiment detection in natural audio streams by using POS technique. 2.4 Sentiment Classiﬁcation Sentiment Classiﬁcation is a task to extraction and classiﬁcation the text whose objective to classify according to a polarity of the opinion it contains (pang 2002), e.g. positive or negative, good or bad, like or dislike. Sentiment classiﬁcation contains multiple techniques, and it is classiﬁed into three main techniques, namely machine learning approach, hybrid techniques approach, and lexicon-based approach [9] and\n",
      "-----------------------------\n",
      "A Study of Sentiment Analysis … 151 words from a large corpus. Hatzivasiloglu [12] is one of the most important methods to represent the corpus-based approach. The ﬁrst step to start was to create a seed list and use it with a wide range of language restrictions to be able to identify additional words including their orientations. To implement corpus-based approach, we use two different approaches: statistical approach and semantic approach as illustrated in the following. i. Statistical approach: It is used in many applications that have a relation in the ﬁeld of SA. The famous of them is the one that can detect the manipulation of the review by conducting a statistical test of randomization which is called runs test [13]. ii. Semantic approach: It gives values to sentiments while relies on more than principle to calculate the afﬁnity and similarity of different words. The basis of this principle is to support the Sentiment value in the words and words close. WorldNet [14]. Dictionary-Based Approach: Dictionary-based approach presented a compre- hensive strategy for the dictionary-based approach. In this famous strategy, a small group of words is hand-picked with known trends [15, 16]. Then, we come to plant this group of words by searching in the known approach corpora thesaurus [17] or WorldNet [18] for all synonyms and antonyms. The new words that are found are added to the seed list, and then, the next repetition begins. This repetitive process continues and stops only when there are no new words. 2.4.2 Machine Learning Approach Machine learning approach is used to solve the problems related to text classiﬁcation that contain syntactic or linguistic features. Whilst lexicon-based approach is used to extracting sentiment from text, it depends on a sentiment lexicon; the collection of known and pre-compiled sentiment terms in Machine Learning algorithms divided intoReinforcementLearning[17],UnsupervisedLearning,andSupervisedLearning. Reinforcement Learning Technique: Its entirety indicates how to make an opti- mal decision an important technique that differs relatively from its counterpart unsu- pervised learning. This technique is highly concerned with improving the efﬁciency of text classiﬁcation to show that the reinforcement learning technique is important and prominent. Unsupervised Approach: It is a unique type of machine learning algorithm and is used in most cases to draw and diverse inferences of data; these groups of data consist of input data without any labeled responses. It is used when it is impossible to obtain labeled training documents. Supervised Learning: It is a type of machine learning approach that uses a data set called training data set to make predictions. These data set contain input data as well as response values. In supervised learning methods, it makes use of a large number of assorted training documents.\n",
      "-----------------------------\n",
      "154 A. A. Q. Aqlan et al. Fig. 3 Knowledge discovery and pattern recognition architecture 3.1 Knowledge Discovery and Pattern Recognition Architecture Social networking sites are many and full of useful data; however, there are important and credible data and useless data (not useful). The reliability data is usually found in cultural sites or shopping sites because the customer may have experience in dealing with commercial sites. So, we have illustrated the kinds of techniques and classiﬁca- tions within the ﬁeld of sentiment analysis and how to extract and manipulate data to reach reliable results. This section will illustrate a knowledge discovery and pattern recognition architecture, Fig. 3. 3.1.1 Social Network The Internet is the right environment and the main source for most information and ideas that are shared by users. It is a resourceful place with respect to sentiment information. By following a lot of ideas and articles, we found that people pre- fer to publish their content through various online social media, such as forums, microblogs, or online social networking sites. Choosing data source is the ﬁrst step in SA. All communication site a fertile environment for data collection; most of\n",
      "-----------------------------\n",
      "A Study of Sentiment Analysis … 157 Use the DBA to build thesaurus lexicon. The purpose of this is to increase the availability of tweet and review for the sentiment classiﬁcation without the need to use human resource. However, accuracy obtained was slightly increased. Ishtiaq Ahsan has build a methodology for reviewing opinions and detecting spam through a well-known learning method (active learning and supervision) with the use of all data including real and fabricated show us very promising results while conducting several different experiments [21]. The results have shown that detection method is very effective and promising. The use of this technique is very convenient for many because it combines two or more technologies, so it shows much better results than other methods. Seongik Park [21] uses the dictionary-based approach to build a lexicon for sen- timent classiﬁcation and uses three online dictionaries rich in vocabulary to collect thesauruses based on the seed words to improved reliability of the lexicon. The- sauruses are a collection of antonyms and synonyms to expand the lexicon more vocabulary. Because he focused only on lexicon building, the result was slightly increased. 5 Big Data Massive amounts of data are stored in communication sites. These data are increasing rapidly every year. Most of the data stored on the Internet has been produced in recent years. But most of the data produced may be useful when processed. Business, political, and social sites are the main source of data growth and the largest data incubator. Big data can be deﬁned as a collection and storage of huge amounts of information for ﬁnal processing; there is a concept that gained momentum among the public in the early 2000s almost by Doug Laney which explain the current mainstream deﬁnition of big data. Big data is of high volume, high velocity, and high variety. Highvolumemeanslargeamountsofinformationarecollectedfromdifferentsources such as commerce, social media, and other communication channels and then stored in Hadoop support. High velocity is a high capability of retrieval data, sharing into slaves, process speed should be very high, high variety means all varieties of data processing, such as structured, unstructured, and semi structured. Big data involves various tools including Hadoop, data science, MongoDB, data mining, Teradata, and Python. Hadoop is a big data framework to store and process high volumes of data with very high speed. Hadoop is an open source. Hadoop satisﬁesallcharacteristicsofbigdata.SoyoucansayHadoopasabigdataframework. Hadoop framework includes two main modules: Hadoop MapReduce and Hadoop Distributed File System. • MapReduce is a processing technique and a program model for distributed com- puting based on Java. MapReduce algorithm involves two important tasks those are map and reduce. The map is A set of data which is converted to another set of\n",
      "-----------------------------\n",
      "158 A. A. Q. Aqlan et al. Fig. 4 Data science terms data (map is input value) Reduce job takes the output from a map; reduce is output of data value, the reduce task is always performed after the map job. • Hadoop Distributed File System (HDFS) is designed to store a huge data sites reliably. HDFS creates several replicas of a huge data for reliability and then put them on compute nodes around the cluster. Data Science: It is very suitable for data analysis, so it can give a boost of improvements to sentiment analysis. Data science is dealing with each structured data, unstructured data, and semi-structured data. In simple terms, it incorporates statistics, mathematical, analysis, signal processing, natural language processing, etc. Data science is an umbrella which involves several terms as illustrated in Fig. 4. 5.1 Big Data Tools in Sentiment Analysis Using the modern program in big data is very important for sentiment analysis at the moment. The use of big data for sentiment is very appropriate and still at the begin- ning of growth. Most of the comments available in social media are unstructured. So, we can use Hadoop technology because Hadoop can deal with structured data, unstructured data, and semi-structured data. A research study is conducted to deter- mine diabetes awareness among different segments of the population using Hadoop MapReduce, [21]. The author Monu Kumar [28] extracts and collect the data from social networks and analyze it by using big data technique. Hadoop MapReduce is processing technique of big data effectively, so priorities use Hadoop for data col-\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "#using spacy to get similarity of context\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "topic = \"data analysis\"\n",
    "topic = nlp(topic)\n",
    "relevant_paragraphs_spacy = []\n",
    "for paragraph in result_paragraphs:\n",
    "    paragraph_tokenized = nlp(paragraph)\n",
    "    print(paragraph_tokenized.similarity(topic))\n",
    "    if paragraph_tokenized.similarity(topic) > 0.6:\n",
    "        relevant_paragraphs_spacy.append(paragraph)\n",
    "print(\"#number of extracted paragraphs: \", len(relevant_paragraphs_spacy))     \n",
    "print (\"-----------------------------\")\n",
    "for paragraph in relevant_paragraphs_spacy:\n",
    "    print (paragraph)\n",
    "    print (\"-----------------------------\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae8c32c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3> <ol> c) Cosine similarity: Feature extraction using bert model </ol></h3>\n",
    "This is the typical ML engineer approach is to build a model for feature extraction to get the context in considerstion and then using similarity measurement to measure similarity of topic features and paragraph features, here I used cosine similarity as a measurement tool for similarity, here I couldn't find a dataset to build a model for feature extraction, that is why I used pre-built model which is BERT model. from my point of view it is the best approach as : - </b>\n",
    "<ol> \n",
    "    <li>  model can be costumised </li> \n",
    "    <li>  context is considered </li> \n",
    "    <li>  can control measurement tool </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "afe6cff6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (4.36.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (2.1.2)\n",
      "Requirement already satisfied: torchvision in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (0.16.2)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (2.1.2)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\cic\\appdata\\roaming\\python\\python39\\site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.11)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "#context extraction and compare the vectors using cosine similiraty for the context\n",
    "!pip install transformers\n",
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "c55f80a8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#number of extracted paragraphs:  7\n",
      "-----------------------------\n",
      "148 A. A. Q. Aqlan et al. Fig. 1 Sentiment analysis process steps paigns rely on knowing the opinions of people through communication sites and see whether they are positive, negative, or neutral. The SA has emerged because of the huge information exchange on the Internet. The SA idea was ﬁrst proposed by Nasukawa [1]. Firstly, the SA is used for natural language process (NLP) [2], which analyzes opinions, feelings, reactions of people and writers on the Internet through social networking sites and business sites about the many products and services. Sentiment analysis is a broad ﬁeld for many researchers and can also be called opin- ion mining; because it helps to classify ideas and opinions as positive, negative, or neutral. SA is a textual study, which is widely used on reviews and surveys in the Internet and social media. It handles responses and customer feedback on commer- cial sites to know their acceptance or rejection of a product; this helps to improve the sales of the company as it tells the choice of a customer. With the explosion of different opinions through social networking sites, new ideas were generated by sys- tems, politicians, psychologists, manufacturers, and researchers to analyze them to implement the best decisions. Sentiment analysis has a high efﬁciency using NLP, as statistics, and machine learning approaches to extract and deﬁne sentiment content in a text unit. 2 Sentiment Analysis Sentiment analysis is becoming very important to study growing opinions faster and faster within social media and other sites, The huge explosion in information in recent years in the sites of communication, air trafﬁc and alternative markets, all this huge amount of information cannot be controlled and analyzed used the traditional way, so the scientists and researchers developed a high-efﬁciency techniques to deal with this data. This requires the SA to process data and know its polarity to determine the right decision. SA involves ﬁve steps to process data; those are data collection, text preparation, sentiment detection, sentiment classiﬁcation, and presentation of output [3] as shown in bellow (Fig. 1).\n",
      "-----------------------------\n",
      "150 A. A. Q. Aqlan et al. Fig. 2 Sentiment classiﬁcation techniques [10]. Presently, Naive Bayes technique and support vector machines (SVMs) are more popular and used for sentiment classiﬁcation. These techniques can improve an accuracy of classiﬁcation of Tweets, such as Ankur Goel [11] use Naive Bayes for Sentiment Analysis Tweets in high speed. Therefore, sentiment analysis has a great deal of research in this ﬁeld and found that many applications and improvements have occurred in sentiment analysis in recent years. In this study, we will clariﬁcation most of the research in this area. These articles cover most of the divisions and classiﬁcations widely in SA ﬁelds. Sentiment classiﬁcation techniques are discussed with more emphasis on most details and related points and originating references. In Fig. 2, we will illustrate all techniques which are currently used in sentiment classiﬁcation until 2017. 2.4.1 Lexicon-Based Approach Multiple words are used to classify sentiment and use positive words for the desired things, while using negative words for undesired things. So, lexicon-based approach relies mainly on ﬁnding opinion lexicon, which is used for text analysis. There are two methods according to lexicon-based approach. The ﬁrst one is corpus-based approach, and the second one is dictionary-based approach. Corpus-Based Approach: The corpus-based approach starts with a seed list of opinion words and then ﬁnds other ideas from the words in a large corpus to get opinions from certain directions. In another meaning, most methods rely on gram- matical patterns or that occur together with the seed list of opinion words to ﬁnd other\n",
      "-----------------------------\n",
      "A Study of Sentiment Analysis … 153 (data) analysis. Use of this technique is very convenient for many because it combines two or more technologies, so it shows much better results than other methods. 2.5 Presentation of Output The main objective of analyzing a huge amount of data is to convert unstructured text into useful information and then to display it through charts such as a graph, line graph, and bar graph. 3 Background SA is contextual mining of texts; it identiﬁes the sentences and subjective information to classify opinions according to polarity. Sentiment analysis studies people’s feel- ings, opinions, assessments, and attitudes toward many services, issues, events, and organizations [17]. SA is not only applied to the commercial product reviews; it can also be applied to all types of social communication sites and stock markets. Three topics work under the umbrella of sentiment analysis emotions detection, building resources, and transfer learning. Emotions detection is a recent ﬁeld of research that is closely related to SA. The aim of SA is to detect positive, negative, or neutral, feelings from the text, whereas emotion detection aims to detect and recognize types of feelings through the expression of texts, such as anger, disgust, fear, happiness, sadness, and surprise. Building resources is a lexicon; it is a vocabulary that is used to express an opinion according to the polarity either positive, negative, or neutral. Transfer learning is considered as the transfer of knowledge from one learned task to a new task in machine learning. The text classiﬁcation according to the following criteria is as follows: The ﬁrst standard is the polarity of sentiment into (positive, negative, or neutral). The second standard is the polarity of the outcome that applies to most political articles and medical facilities for managing disease data as follows [19]: • Use agree or disagree, e.g., political debates [20]; • Criteria good or bad [21]; • Pros and cons: The meaning of this is either positive or negative [22]; in the following ﬁgure, will clarify steps of Sentiment Classiﬁcation and related works in SA.\n",
      "-----------------------------\n",
      "154 A. A. Q. Aqlan et al. Fig. 3 Knowledge discovery and pattern recognition architecture 3.1 Knowledge Discovery and Pattern Recognition Architecture Social networking sites are many and full of useful data; however, there are important and credible data and useless data (not useful). The reliability data is usually found in cultural sites or shopping sites because the customer may have experience in dealing with commercial sites. So, we have illustrated the kinds of techniques and classiﬁca- tions within the ﬁeld of sentiment analysis and how to extract and manipulate data to reach reliable results. This section will illustrate a knowledge discovery and pattern recognition architecture, Fig. 3. 3.1.1 Social Network The Internet is the right environment and the main source for most information and ideas that are shared by users. It is a resourceful place with respect to sentiment information. By following a lot of ideas and articles, we found that people pre- fer to publish their content through various online social media, such as forums, microblogs, or online social networking sites. Choosing data source is the ﬁrst step in SA. All communication site a fertile environment for data collection; most of\n",
      "-----------------------------\n",
      "A Study of Sentiment Analysis … 155 the data is useful, and often, there is abusive data which not useful, these data are excluded automatically during analysis. 3.1.2 Data Collection Application program interface (API)—this is the proposed system to extract the data and download. It is characterized by research for hashtags, main keywords, and other classiﬁers simultaneously [23]. API is widely used to collect reviews by researchers and interested companies, but now we can use different Application program to collect data such as Hadoop ﬂume, Shirahatti [21]. 3.1.3 Natural Language Processing Natural Language Processing: It deals with all human languages whether written or oral to process and apply. This is the main propose of NLP. NLP refers to part of text; part of text includes verbs, adjectives, and nouns. We may get unorganized data in this case and need further processing using one of the algorithms (part of speech or N gram). Part of Speech can divide the sentences to small words, and each word has a meaning; for example, in English language each small word can have a distinctive meaning and this comes according to its use and functions, and is categorized into several types or small parts of speech such as noun, pronoun, verb, adverb, adjective, conjunction, preposition, and interjection. N Gram, An n gram is simply a sequence of tokens. In the context of computa- tional linguistics, these tokens are usually words, though they can be characters or subsets of characters. The n simply refers to the number of tokens. n gram is used for word sequence itself or predictive model that assigns it a probability. The gram is a combination of letters; n gram refers to divide the sentence into several parts (count the word); for example, “Friday will be holiday”—this is contains 4 gram; “tomor- row is holiday”—this is contains 3 gram. Named-entity recognition (NER), using to divides comments or the tweet into smaller parts this parts each one containing two words. 3.1.4 Preprocessing Preprocessing relies mainly on ﬁnding opinion lexicon, which is used for text anal- ysis; it classiﬁes the words either positive, negative, or neutral. 3.1.5 Feature Identiﬁer Feature identiﬁer: It is to identify and classify the entities being referred to as tweets. Final stage SVM classiﬁer is trained in order to obtain the tweet’s label.\n",
      "-----------------------------\n",
      "156 A. A. Q. Aqlan et al. 3.1.6 Classiﬁer Approach Classiﬁer Approach: It is used to analyze datasets for classiﬁcation and regression analysis. It classiﬁes the opinion positive, negative, or neutral in a ﬁnal stage. 3.1.7 Result Analyzation Result Analyzation or sentiment analysis commonly uses several ratings to express the abundance of feelings or versa. Sentiment is evaluated through the use of stars; some Websites require the evaluation of their material by using stars, and we will illustrate the rating of stars commonly used as follows [24] and [25]. • Positive +2 or 5 stars • Rational positive +1 or 4 stars • Neutral 0 or 3 stars • Rational negative –1 or 2 stars • Negative –2 or 1 star 4 Related Work The purpose of this study is to give a clear conception of most techniques in the ﬁeld of sentiment analysis, where it is easier for new researchers to beneﬁt from it. As mentioned many techniques of analysis, we will clarify some studies and research in recent years that dealt with this area; this paper also covers a wide ﬁeld of sentiment classiﬁcation technique and approach in SA ﬁeld. Lexicon-based technique aims to extract and collect data from social network such as Twitter [25], Facebook, etc. by use API Graph to collect and load all the target data for analysis, and examine all words that do not represent an emotional value or feature, then created a list of words and analyzed them, that would be used in all cases, these shown positive results in predict the sentiment behind a status post on Facebook by use lexicon-based approach with high efﬁciency. Machine learning approach is not limited to the analysis of data in social media, Where used to know the driver’s sense at the moment of leadership. One of them sought to generate and know the rules of the cognitive deviations of the drivers directly from the place of the driving simulation environment. Through this study, the eye movements of the drivers were taken using a simulated device [26]. Dictionary-based approach is used with high efﬁciency in the ﬁeld of SA. Seongik Park build thesaurus lexicon characterized in clearly and credibility [27], Where build this approach through three online dictionaries to gathering thesauruses based on the seed words, and sought to stores the real words which can be trusted into the thesaurus lexicon in order to improve the reputation and credibility of the thesaurus lexicon, and prove it a prominent lexicon.\n",
      "-----------------------------\n",
      "158 A. A. Q. Aqlan et al. Fig. 4 Data science terms data (map is input value) Reduce job takes the output from a map; reduce is output of data value, the reduce task is always performed after the map job. • Hadoop Distributed File System (HDFS) is designed to store a huge data sites reliably. HDFS creates several replicas of a huge data for reliability and then put them on compute nodes around the cluster. Data Science: It is very suitable for data analysis, so it can give a boost of improvements to sentiment analysis. Data science is dealing with each structured data, unstructured data, and semi-structured data. In simple terms, it incorporates statistics, mathematical, analysis, signal processing, natural language processing, etc. Data science is an umbrella which involves several terms as illustrated in Fig. 4. 5.1 Big Data Tools in Sentiment Analysis Using the modern program in big data is very important for sentiment analysis at the moment. The use of big data for sentiment is very appropriate and still at the begin- ning of growth. Most of the comments available in social media are unstructured. So, we can use Hadoop technology because Hadoop can deal with structured data, unstructured data, and semi-structured data. A research study is conducted to deter- mine diabetes awareness among different segments of the population using Hadoop MapReduce, [21]. The author Monu Kumar [28] extracts and collect the data from social networks and analyze it by using big data technique. Hadoop MapReduce is processing technique of big data effectively, so priorities use Hadoop for data col-\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "#using bert model\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "topic = \"data analysis\"\n",
    "\n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\",padding=True, truncation=True,max_length=50, add_special_tokens = True)\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)  #using mean pooling for sentence embeddings\n",
    "    return embeddings\n",
    "\n",
    "#this is for vector cosine similarity\n",
    "def compute_cosine_similarity(embeddings1, embeddings2):\n",
    "    similarity = torch.nn.functional.cosine_similarity(embeddings1, embeddings2)\n",
    "    return similarity.item()\n",
    "\n",
    "topic_embeddings = get_bert_embeddings(topic)\n",
    "relevant_paragraphs = []\n",
    "for paragraph in result_paragraphs:\n",
    "    paragraph_embeddings = get_bert_embeddings(paragraph)\n",
    "    similarity_matrix = compute_cosine_similarity(topic_embeddings, paragraph_embeddings)\n",
    "    if similarity_matrix > 0.4:\n",
    "        relevant_paragraphs.append(paragraph)\n",
    "        \n",
    "print(\"#number of extracted paragraphs: \", len(relevant_paragraphs))     \n",
    "print (\"-----------------------------\")\n",
    "for paragraph in relevant_paragraphs:\n",
    "    print (paragraph)\n",
    "    print (\"-----------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a0fb12",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3> <ol>3. Headline generation </ol></h3>\n",
    "\n",
    "<b> the last step is to generate headline for each extracted paragraph, I decided to split the paragraph into multiple sentences and use one of them as a headline, I had two approaches :\n",
    "<ol> \n",
    "    <ol> a) using the sentence that has most frequence words inside the paragraph </ol> \n",
    "    <ol> b) using cosine similarity </ol> \n",
    "</ol>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b18023f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3> <ol> a) using the sentence that has most frequence words inside the paragraph </ol></h3>\n",
    "In this approach I need to do some preprocessing first as we depend on word occurence so I first need to remove numbers and special characters to avoid errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "1fcd2419",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'headline': ' A  A  Q  Aqlan et al  Fig   Sentiment analysis process steps paigns rely on knowing the opinions of people through communication sites and see whether they are positive  negative  or neutral  The SA has emerged because of the huge information exchange on the Internet  The SA idea was ﬁrst proposed by Nasukawa     Firstly  the SA is used for natural language process  NLP      which analyzes opinions  feelings  reactions of people and writers on the Internet through social networking sites and business sites about the many products and services  Sentiment analysis is a broad ﬁeld for many researchers and can also be called opin  ion mining  because it helps to classify ideas and opinions as positive  negative  or neutral  SA is a textual study  which is widely used on reviews and surveys in the Internet and social media  It handles responses and customer feedback on commer  cial sites to know their acceptance or rejection of a product  this helps to improve the sales of the company as it tells the choice of a customer  With the explosion of different opinions through social networking sites  new ideas were generated by sys  tems  politicians  psychologists  manufacturers  and researchers to analyze them to implement the best decisions  Sentiment analysis has a high efﬁciency using NLP  as statistics  and machine learning approaches to extract and deﬁne sentiment content in a text unit   Sentiment Analysis Sentiment analysis is becoming very important to study growing opinions faster and faster within social media and other sites  The huge explosion in information in recent years in the sites of communication  air trafﬁc and alternative markets  all this huge amount of information cannot be controlled and analyzed used the traditional way  so the scientists and researchers developed a high efﬁciency techniques to deal with this data  This requires the SA to process data and know its polarity to determine the right decision  SA involves ﬁve steps to process data  those are data collection  text preparation  sentiment detection  sentiment classiﬁcation  and presentation of output    as shown in bellow  Fig    ', 'paragraph': '148 A. A. Q. Aqlan et al. Fig. 1 Sentiment analysis process steps paigns rely on knowing the opinions of people through communication sites and see whether they are positive, negative, or neutral. The SA has emerged because of the huge information exchange on the Internet. The SA idea was ﬁrst proposed by Nasukawa [1]. Firstly, the SA is used for natural language process (NLP) [2], which analyzes opinions, feelings, reactions of people and writers on the Internet through social networking sites and business sites about the many products and services. Sentiment analysis is a broad ﬁeld for many researchers and can also be called opin- ion mining; because it helps to classify ideas and opinions as positive, negative, or neutral. SA is a textual study, which is widely used on reviews and surveys in the Internet and social media. It handles responses and customer feedback on commer- cial sites to know their acceptance or rejection of a product; this helps to improve the sales of the company as it tells the choice of a customer. With the explosion of different opinions through social networking sites, new ideas were generated by sys- tems, politicians, psychologists, manufacturers, and researchers to analyze them to implement the best decisions. Sentiment analysis has a high efﬁciency using NLP, as statistics, and machine learning approaches to extract and deﬁne sentiment content in a text unit. 2 Sentiment Analysis Sentiment analysis is becoming very important to study growing opinions faster and faster within social media and other sites, The huge explosion in information in recent years in the sites of communication, air trafﬁc and alternative markets, all this huge amount of information cannot be controlled and analyzed used the traditional way, so the scientists and researchers developed a high-efﬁciency techniques to deal with this data. This requires the SA to process data and know its polarity to determine the right decision. SA involves ﬁve steps to process data; those are data collection, text preparation, sentiment detection, sentiment classiﬁcation, and presentation of output [3] as shown in bellow (Fig. 1).'}, {'headline': 'In this study  we will clariﬁcation most of the research in this area  These articles cover most of the divisions and classiﬁcations widely in SA ﬁelds  Sentiment classiﬁcation techniques are discussed with more emphasis on most details and related points and originating references  In Fig    we will illustrate all techniques which are currently used in sentiment classiﬁcation until      Lexicon Based Approach Multiple words are used to classify sentiment and use positive words for the desired things  while using negative words for undesired things  So  lexicon based approach relies mainly on ﬁnding opinion lexicon  which is used for text analysis  There are two methods according to lexicon based approach  The ﬁrst one is corpus based approach  and the second one is', 'paragraph': '150 A. A. Q. Aqlan et al. Fig. 2 Sentiment classiﬁcation techniques [10]. Presently, Naive Bayes technique and support vector machines (SVMs) are more popular and used for sentiment classiﬁcation. These techniques can improve an accuracy of classiﬁcation of Tweets, such as Ankur Goel [11] use Naive Bayes for Sentiment Analysis Tweets in high speed. Therefore, sentiment analysis has a great deal of research in this ﬁeld and found that many applications and improvements have occurred in sentiment analysis in recent years. In this study, we will clariﬁcation most of the research in this area. These articles cover most of the divisions and classiﬁcations widely in SA ﬁelds. Sentiment classiﬁcation techniques are discussed with more emphasis on most details and related points and originating references. In Fig. 2, we will illustrate all techniques which are currently used in sentiment classiﬁcation until 2017. 2.4.1 Lexicon-Based Approach Multiple words are used to classify sentiment and use positive words for the desired things, while using negative words for undesired things. So, lexicon-based approach relies mainly on ﬁnding opinion lexicon, which is used for text analysis. There are two methods according to lexicon-based approach. The ﬁrst one is corpus-based approach, and the second one is dictionary-based approach. Corpus-Based Approach: The corpus-based approach starts with a seed list of opinion words and then ﬁnds other ideas from the words in a large corpus to get opinions from certain directions. In another meaning, most methods rely on gram- matical patterns or that occur together with the seed list of opinion words to ﬁnd other'}, {'headline': 'A Study of Sentiment Analysis     data  analysis  Use of this technique is very convenient for many because it combines two or more technologies  so it shows much better results than other methods    Presentation of Output The main objective of analyzing a huge amount of data is to convert unstructured text into useful information and then to display it through charts such as a graph  line graph  and bar graph   Background SA is contextual mining of texts  it identiﬁes the sentences and subjective information to classify opinions according to polarity  Sentiment analysis studies people s feel  ings  opinions  assessments  and attitudes toward many services  issues  events  and organizations     SA is not only applied to the commercial product reviews  it can also be applied to all types of social communication sites and stock markets  Three topics work under the umbrella of sentiment analysis emotions detection  building resources  and transfer learning  Emotions detection is a recent ﬁeld of research that is closely related to SA  ', 'paragraph': 'A Study of Sentiment Analysis … 153 (data) analysis. Use of this technique is very convenient for many because it combines two or more technologies, so it shows much better results than other methods. 2.5 Presentation of Output The main objective of analyzing a huge amount of data is to convert unstructured text into useful information and then to display it through charts such as a graph, line graph, and bar graph. 3 Background SA is contextual mining of texts; it identiﬁes the sentences and subjective information to classify opinions according to polarity. Sentiment analysis studies people’s feel- ings, opinions, assessments, and attitudes toward many services, issues, events, and organizations [17]. SA is not only applied to the commercial product reviews; it can also be applied to all types of social communication sites and stock markets. Three topics work under the umbrella of sentiment analysis emotions detection, building resources, and transfer learning. Emotions detection is a recent ﬁeld of research that is closely related to SA. The aim of SA is to detect positive, negative, or neutral, feelings from the text, whereas emotion detection aims to detect and recognize types of feelings through the expression of texts, such as anger, disgust, fear, happiness, sadness, and surprise. Building resources is a lexicon; it is a vocabulary that is used to express an opinion according to the polarity either positive, negative, or neutral. Transfer learning is considered as the transfer of knowledge from one learned task to a new task in machine learning. The text classiﬁcation according to the following criteria is as follows: The ﬁrst standard is the polarity of sentiment into (positive, negative, or neutral). The second standard is the polarity of the outcome that applies to most political articles and medical facilities for managing disease data as follows [19]: • Use agree or disagree, e.g., political debates [20]; • Criteria good or bad [21]; • Pros and cons: The meaning of this is either positive or negative [22]; in the following ﬁgure, will clarify steps of Sentiment Classiﬁcation and related works in SA.'}, {'headline': 'The reliability data is usually found in cultural sites or shopping sites because the customer may have experience in dealing with commercial sites  So  we have illustrated the kinds of techniques and classiﬁca  tions within the ﬁeld of sentiment analysis and how to extract and manipulate data to reach reliable results  ', 'paragraph': '154 A. A. Q. Aqlan et al. Fig. 3 Knowledge discovery and pattern recognition architecture 3.1 Knowledge Discovery and Pattern Recognition Architecture Social networking sites are many and full of useful data; however, there are important and credible data and useless data (not useful). The reliability data is usually found in cultural sites or shopping sites because the customer may have experience in dealing with commercial sites. So, we have illustrated the kinds of techniques and classiﬁca- tions within the ﬁeld of sentiment analysis and how to extract and manipulate data to reach reliable results. This section will illustrate a knowledge discovery and pattern recognition architecture, Fig. 3. 3.1.1 Social Network The Internet is the right environment and the main source for most information and ideas that are shared by users. It is a resourceful place with respect to sentiment information. By following a lot of ideas and articles, we found that people pre- fer to publish their content through various online social media, such as forums, microblogs, or online social networking sites. Choosing data source is the ﬁrst step in SA. All communication site a fertile environment for data collection; most of'}, {'headline': 'this is contains  gram   tomor  row is holiday  ', 'paragraph': 'A Study of Sentiment Analysis … 155 the data is useful, and often, there is abusive data which not useful, these data are excluded automatically during analysis. 3.1.2 Data Collection Application program interface (API)—this is the proposed system to extract the data and download. It is characterized by research for hashtags, main keywords, and other classiﬁers simultaneously [23]. API is widely used to collect reviews by researchers and interested companies, but now we can use different Application program to collect data such as Hadoop ﬂume, Shirahatti [21]. 3.1.3 Natural Language Processing Natural Language Processing: It deals with all human languages whether written or oral to process and apply. This is the main propose of NLP. NLP refers to part of text; part of text includes verbs, adjectives, and nouns. We may get unorganized data in this case and need further processing using one of the algorithms (part of speech or N gram). Part of Speech can divide the sentences to small words, and each word has a meaning; for example, in English language each small word can have a distinctive meaning and this comes according to its use and functions, and is categorized into several types or small parts of speech such as noun, pronoun, verb, adverb, adjective, conjunction, preposition, and interjection. N Gram, An n gram is simply a sequence of tokens. In the context of computa- tional linguistics, these tokens are usually words, though they can be characters or subsets of characters. The n simply refers to the number of tokens. n gram is used for word sequence itself or predictive model that assigns it a probability. The gram is a combination of letters; n gram refers to divide the sentence into several parts (count the word); for example, “Friday will be holiday”—this is contains 4 gram; “tomor- row is holiday”—this is contains 3 gram. Named-entity recognition (NER), using to divides comments or the tweet into smaller parts this parts each one containing two words. 3.1.4 Preprocessing Preprocessing relies mainly on ﬁnding opinion lexicon, which is used for text anal- ysis; it classiﬁes the words either positive, negative, or neutral. 3.1.5 Feature Identiﬁer Feature identiﬁer: It is to identify and classify the entities being referred to as tweets. Final stage SVM classiﬁer is trained in order to obtain the tweet’s label.'}, {'headline': ' A  A  Q  Aqlan et al     Classiﬁer Approach Classiﬁer Approach  ', 'paragraph': '156 A. A. Q. Aqlan et al. 3.1.6 Classiﬁer Approach Classiﬁer Approach: It is used to analyze datasets for classiﬁcation and regression analysis. It classiﬁes the opinion positive, negative, or neutral in a ﬁnal stage. 3.1.7 Result Analyzation Result Analyzation or sentiment analysis commonly uses several ratings to express the abundance of feelings or versa. Sentiment is evaluated through the use of stars; some Websites require the evaluation of their material by using stars, and we will illustrate the rating of stars commonly used as follows [24] and [25]. • Positive +2 or 5 stars • Rational positive +1 or 4 stars • Neutral 0 or 3 stars • Rational negative –1 or 2 stars • Negative –2 or 1 star 4 Related Work The purpose of this study is to give a clear conception of most techniques in the ﬁeld of sentiment analysis, where it is easier for new researchers to beneﬁt from it. As mentioned many techniques of analysis, we will clarify some studies and research in recent years that dealt with this area; this paper also covers a wide ﬁeld of sentiment classiﬁcation technique and approach in SA ﬁeld. Lexicon-based technique aims to extract and collect data from social network such as Twitter [25], Facebook, etc. by use API Graph to collect and load all the target data for analysis, and examine all words that do not represent an emotional value or feature, then created a list of words and analyzed them, that would be used in all cases, these shown positive results in predict the sentiment behind a status post on Facebook by use lexicon-based approach with high efﬁciency. Machine learning approach is not limited to the analysis of data in social media, Where used to know the driver’s sense at the moment of leadership. One of them sought to generate and know the rules of the cognitive deviations of the drivers directly from the place of the driving simulation environment. Through this study, the eye movements of the drivers were taken using a simulated device [26]. Dictionary-based approach is used with high efﬁciency in the ﬁeld of SA. Seongik Park build thesaurus lexicon characterized in clearly and credibility [27], Where build this approach through three online dictionaries to gathering thesauruses based on the seed words, and sought to stores the real words which can be trusted into the thesaurus lexicon in order to improve the reputation and credibility of the thesaurus lexicon, and prove it a prominent lexicon.'}, {'headline': 'It is very suitable for data analysis  so it can give a boost of improvements to sentiment analysis  Data science is dealing with each structured data  unstructured data  and semi structured data  ', 'paragraph': '158 A. A. Q. Aqlan et al. Fig. 4 Data science terms data (map is input value) Reduce job takes the output from a map; reduce is output of data value, the reduce task is always performed after the map job. • Hadoop Distributed File System (HDFS) is designed to store a huge data sites reliably. HDFS creates several replicas of a huge data for reliability and then put them on compute nodes around the cluster. Data Science: It is very suitable for data analysis, so it can give a boost of improvements to sentiment analysis. Data science is dealing with each structured data, unstructured data, and semi-structured data. In simple terms, it incorporates statistics, mathematical, analysis, signal processing, natural language processing, etc. Data science is an umbrella which involves several terms as illustrated in Fig. 4. 5.1 Big Data Tools in Sentiment Analysis Using the modern program in big data is very important for sentiment analysis at the moment. The use of big data for sentiment is very appropriate and still at the begin- ning of growth. Most of the comments available in social media are unstructured. So, we can use Hadoop technology because Hadoop can deal with structured data, unstructured data, and semi-structured data. A research study is conducted to deter- mine diabetes awareness among different segments of the population using Hadoop MapReduce, [21]. The author Monu Kumar [28] extracts and collect the data from social networks and analyze it by using big data technique. Hadoop MapReduce is processing technique of big data effectively, so priorities use Hadoop for data col-'}]\n"
     ]
    }
   ],
   "source": [
    "#extractive summary by word count in each sentence\n",
    "import spacy\n",
    " \n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "relevant_paragraphs_with_occurence=[]\n",
    "for paragraph in relevant_paragraphs:\n",
    "    #preprocess\n",
    "    #match all digits in the string and replace them with an empty string\n",
    "    processed_feature = re.sub(r'[0-9]+', '', paragraph)    \n",
    "    #remove all the special characters\n",
    "    processed_feature = re.sub(r'\\W', ' ', processed_feature)\n",
    "    doc = nlp(processed_feature)\n",
    "    word_dict = {}\n",
    "    #loop through every sentence and give it a weight\n",
    "    for word in doc:\n",
    "        word = word.text.lower()\n",
    "        if word in word_dict:\n",
    "            word_dict[word] += 1\n",
    "        else:\n",
    "            word_dict[word] = 1\n",
    "    #list of tuple (sentence text, score, index)\n",
    "    sents = []\n",
    "    sent_score = 0\n",
    "    for index, sent in enumerate(doc.sents):\n",
    "        for word in sent:\n",
    "            word = word.text.lower()\n",
    "            sent_score += word_dict[word]\n",
    "        sents.append((sent.text.replace(\"\\n\", \" \"), sent_score/len(sent), index))\n",
    "    #sort sentence by word occurrences\n",
    "    sents = sorted(sents, key=lambda x: -x[1])\n",
    "    #summarize in 3 sentences \n",
    "    sents = sorted(sents[:3], key=lambda x: x[2])\n",
    "\n",
    "    #compile them into text\n",
    "    headline = sents[0][0]\n",
    "    relevant_paragraphs_with_occurence.append({\"headline\":headline,\"paragraph\":paragraph})\n",
    "print(relevant_paragraphs_with_occurence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe60800",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h3> <ol> b) using cosine similarity </ol></h3>\n",
    "this approach is similar to the one I used at the beggining of that notebook, but here I am splitting paragraph sentneces, and compare similarity of each sentence to the whole paragraph, and then I use the most similar sentence as the headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "abf50ca8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'headline': ' Sentiment analysis process steps paigns rely on knowing the opinions of people through communication sites and see whether they are positive  negative  or neutral ', 'paragraph': '148 A. A. Q. Aqlan et al. Fig. 1 Sentiment analysis process steps paigns rely on knowing the opinions of people through communication sites and see whether they are positive, negative, or neutral. The SA has emerged because of the huge information exchange on the Internet. The SA idea was ﬁrst proposed by Nasukawa [1]. Firstly, the SA is used for natural language process (NLP) [2], which analyzes opinions, feelings, reactions of people and writers on the Internet through social networking sites and business sites about the many products and services. Sentiment analysis is a broad ﬁeld for many researchers and can also be called opin- ion mining; because it helps to classify ideas and opinions as positive, negative, or neutral. SA is a textual study, which is widely used on reviews and surveys in the Internet and social media. It handles responses and customer feedback on commer- cial sites to know their acceptance or rejection of a product; this helps to improve the sales of the company as it tells the choice of a customer. With the explosion of different opinions through social networking sites, new ideas were generated by sys- tems, politicians, psychologists, manufacturers, and researchers to analyze them to implement the best decisions. Sentiment analysis has a high efﬁciency using NLP, as statistics, and machine learning approaches to extract and deﬁne sentiment content in a text unit. 2 Sentiment Analysis Sentiment analysis is becoming very important to study growing opinions faster and faster within social media and other sites, The huge explosion in information in recent years in the sites of communication, air trafﬁc and alternative markets, all this huge amount of information cannot be controlled and analyzed used the traditional way, so the scientists and researchers developed a high-efﬁciency techniques to deal with this data. This requires the SA to process data and know its polarity to determine the right decision. SA involves ﬁve steps to process data; those are data collection, text preparation, sentiment detection, sentiment classiﬁcation, and presentation of output [3] as shown in bellow (Fig. 1).'}, {'headline': 'Presently  Naive Bayes technique and support vector machines  SVMs  are more popular and used for sentiment classiﬁcation ', 'paragraph': '150 A. A. Q. Aqlan et al. Fig. 2 Sentiment classiﬁcation techniques [10]. Presently, Naive Bayes technique and support vector machines (SVMs) are more popular and used for sentiment classiﬁcation. These techniques can improve an accuracy of classiﬁcation of Tweets, such as Ankur Goel [11] use Naive Bayes for Sentiment Analysis Tweets in high speed. Therefore, sentiment analysis has a great deal of research in this ﬁeld and found that many applications and improvements have occurred in sentiment analysis in recent years. In this study, we will clariﬁcation most of the research in this area. These articles cover most of the divisions and classiﬁcations widely in SA ﬁelds. Sentiment classiﬁcation techniques are discussed with more emphasis on most details and related points and originating references. In Fig. 2, we will illustrate all techniques which are currently used in sentiment classiﬁcation until 2017. 2.4.1 Lexicon-Based Approach Multiple words are used to classify sentiment and use positive words for the desired things, while using negative words for undesired things. So, lexicon-based approach relies mainly on ﬁnding opinion lexicon, which is used for text analysis. There are two methods according to lexicon-based approach. The ﬁrst one is corpus-based approach, and the second one is dictionary-based approach. Corpus-Based Approach: The corpus-based approach starts with a seed list of opinion words and then ﬁnds other ideas from the words in a large corpus to get opinions from certain directions. In another meaning, most methods rely on gram- matical patterns or that occur together with the seed list of opinion words to ﬁnd other'}, {'headline': 'Use of this technique is very convenient for many because it combines two or more technologies  so it shows much better results than other methods ', 'paragraph': 'A Study of Sentiment Analysis … 153 (data) analysis. Use of this technique is very convenient for many because it combines two or more technologies, so it shows much better results than other methods. 2.5 Presentation of Output The main objective of analyzing a huge amount of data is to convert unstructured text into useful information and then to display it through charts such as a graph, line graph, and bar graph. 3 Background SA is contextual mining of texts; it identiﬁes the sentences and subjective information to classify opinions according to polarity. Sentiment analysis studies people’s feel- ings, opinions, assessments, and attitudes toward many services, issues, events, and organizations [17]. SA is not only applied to the commercial product reviews; it can also be applied to all types of social communication sites and stock markets. Three topics work under the umbrella of sentiment analysis emotions detection, building resources, and transfer learning. Emotions detection is a recent ﬁeld of research that is closely related to SA. The aim of SA is to detect positive, negative, or neutral, feelings from the text, whereas emotion detection aims to detect and recognize types of feelings through the expression of texts, such as anger, disgust, fear, happiness, sadness, and surprise. Building resources is a lexicon; it is a vocabulary that is used to express an opinion according to the polarity either positive, negative, or neutral. Transfer learning is considered as the transfer of knowledge from one learned task to a new task in machine learning. The text classiﬁcation according to the following criteria is as follows: The ﬁrst standard is the polarity of sentiment into (positive, negative, or neutral). The second standard is the polarity of the outcome that applies to most political articles and medical facilities for managing disease data as follows [19]: • Use agree or disagree, e.g., political debates [20]; • Criteria good or bad [21]; • Pros and cons: The meaning of this is either positive or negative [22]; in the following ﬁgure, will clarify steps of Sentiment Classiﬁcation and related works in SA.'}, {'headline': ' Knowledge discovery and pattern recognition architecture   Knowledge Discovery and Pattern Recognition Architecture Social networking sites are many and full of useful data  however  there are important and credible data and useless data  not useful  ', 'paragraph': '154 A. A. Q. Aqlan et al. Fig. 3 Knowledge discovery and pattern recognition architecture 3.1 Knowledge Discovery and Pattern Recognition Architecture Social networking sites are many and full of useful data; however, there are important and credible data and useless data (not useful). The reliability data is usually found in cultural sites or shopping sites because the customer may have experience in dealing with commercial sites. So, we have illustrated the kinds of techniques and classiﬁca- tions within the ﬁeld of sentiment analysis and how to extract and manipulate data to reach reliable results. This section will illustrate a knowledge discovery and pattern recognition architecture, Fig. 3. 3.1.1 Social Network The Internet is the right environment and the main source for most information and ideas that are shared by users. It is a resourceful place with respect to sentiment information. By following a lot of ideas and articles, we found that people pre- fer to publish their content through various online social media, such as forums, microblogs, or online social networking sites. Choosing data source is the ﬁrst step in SA. All communication site a fertile environment for data collection; most of'}, {'headline': 'A Study of Sentiment Analysis    the data is useful  and often  there is abusive data which not useful  these data are excluded automatically during analysis ', 'paragraph': 'A Study of Sentiment Analysis … 155 the data is useful, and often, there is abusive data which not useful, these data are excluded automatically during analysis. 3.1.2 Data Collection Application program interface (API)—this is the proposed system to extract the data and download. It is characterized by research for hashtags, main keywords, and other classiﬁers simultaneously [23]. API is widely used to collect reviews by researchers and interested companies, but now we can use different Application program to collect data such as Hadoop ﬂume, Shirahatti [21]. 3.1.3 Natural Language Processing Natural Language Processing: It deals with all human languages whether written or oral to process and apply. This is the main propose of NLP. NLP refers to part of text; part of text includes verbs, adjectives, and nouns. We may get unorganized data in this case and need further processing using one of the algorithms (part of speech or N gram). Part of Speech can divide the sentences to small words, and each word has a meaning; for example, in English language each small word can have a distinctive meaning and this comes according to its use and functions, and is categorized into several types or small parts of speech such as noun, pronoun, verb, adverb, adjective, conjunction, preposition, and interjection. N Gram, An n gram is simply a sequence of tokens. In the context of computa- tional linguistics, these tokens are usually words, though they can be characters or subsets of characters. The n simply refers to the number of tokens. n gram is used for word sequence itself or predictive model that assigns it a probability. The gram is a combination of letters; n gram refers to divide the sentence into several parts (count the word); for example, “Friday will be holiday”—this is contains 4 gram; “tomor- row is holiday”—this is contains 3 gram. Named-entity recognition (NER), using to divides comments or the tweet into smaller parts this parts each one containing two words. 3.1.4 Preprocessing Preprocessing relies mainly on ﬁnding opinion lexicon, which is used for text anal- ysis; it classiﬁes the words either positive, negative, or neutral. 3.1.5 Feature Identiﬁer Feature identiﬁer: It is to identify and classify the entities being referred to as tweets. Final stage SVM classiﬁer is trained in order to obtain the tweet’s label.'}, {'headline': '   Classiﬁer Approach Classiﬁer Approach  It is used to analyze datasets for classiﬁcation and regression analysis ', 'paragraph': '156 A. A. Q. Aqlan et al. 3.1.6 Classiﬁer Approach Classiﬁer Approach: It is used to analyze datasets for classiﬁcation and regression analysis. It classiﬁes the opinion positive, negative, or neutral in a ﬁnal stage. 3.1.7 Result Analyzation Result Analyzation or sentiment analysis commonly uses several ratings to express the abundance of feelings or versa. Sentiment is evaluated through the use of stars; some Websites require the evaluation of their material by using stars, and we will illustrate the rating of stars commonly used as follows [24] and [25]. • Positive +2 or 5 stars • Rational positive +1 or 4 stars • Neutral 0 or 3 stars • Rational negative –1 or 2 stars • Negative –2 or 1 star 4 Related Work The purpose of this study is to give a clear conception of most techniques in the ﬁeld of sentiment analysis, where it is easier for new researchers to beneﬁt from it. As mentioned many techniques of analysis, we will clarify some studies and research in recent years that dealt with this area; this paper also covers a wide ﬁeld of sentiment classiﬁcation technique and approach in SA ﬁeld. Lexicon-based technique aims to extract and collect data from social network such as Twitter [25], Facebook, etc. by use API Graph to collect and load all the target data for analysis, and examine all words that do not represent an emotional value or feature, then created a list of words and analyzed them, that would be used in all cases, these shown positive results in predict the sentiment behind a status post on Facebook by use lexicon-based approach with high efﬁciency. Machine learning approach is not limited to the analysis of data in social media, Where used to know the driver’s sense at the moment of leadership. One of them sought to generate and know the rules of the cognitive deviations of the drivers directly from the place of the driving simulation environment. Through this study, the eye movements of the drivers were taken using a simulated device [26]. Dictionary-based approach is used with high efﬁciency in the ﬁeld of SA. Seongik Park build thesaurus lexicon characterized in clearly and credibility [27], Where build this approach through three online dictionaries to gathering thesauruses based on the seed words, and sought to stores the real words which can be trusted into the thesaurus lexicon in order to improve the reputation and credibility of the thesaurus lexicon, and prove it a prominent lexicon.'}, {'headline': ' Data science terms data  map is input value  Reduce job takes the output from a map  reduce is output of data value  the reduce task is always performed after the map job ', 'paragraph': '158 A. A. Q. Aqlan et al. Fig. 4 Data science terms data (map is input value) Reduce job takes the output from a map; reduce is output of data value, the reduce task is always performed after the map job. • Hadoop Distributed File System (HDFS) is designed to store a huge data sites reliably. HDFS creates several replicas of a huge data for reliability and then put them on compute nodes around the cluster. Data Science: It is very suitable for data analysis, so it can give a boost of improvements to sentiment analysis. Data science is dealing with each structured data, unstructured data, and semi-structured data. In simple terms, it incorporates statistics, mathematical, analysis, signal processing, natural language processing, etc. Data science is an umbrella which involves several terms as illustrated in Fig. 4. 5.1 Big Data Tools in Sentiment Analysis Using the modern program in big data is very important for sentiment analysis at the moment. The use of big data for sentiment is very appropriate and still at the begin- ning of growth. Most of the comments available in social media are unstructured. So, we can use Hadoop technology because Hadoop can deal with structured data, unstructured data, and semi-structured data. A research study is conducted to deter- mine diabetes awareness among different segments of the population using Hadoop MapReduce, [21]. The author Monu Kumar [28] extracts and collect the data from social networks and analyze it by using big data technique. Hadoop MapReduce is processing technique of big data effectively, so priorities use Hadoop for data col-'}]\n"
     ]
    }
   ],
   "source": [
    "#comparing similarity to overall paragraph approach\n",
    "relevant_paragraphs_with_topics_similarity=[]\n",
    "for paragraph in relevant_paragraphs:\n",
    "    sentences = sent_tokenize(paragraph)\n",
    "    headline = \"\"\n",
    "    headline_similarity = 0\n",
    "    for sentence in sentences:\n",
    "        #match all digits in the string and replace them with an empty string\n",
    "        sentence = re.sub(r'[0-9]+', '', sentence)\n",
    "        #remove all the special characters\n",
    "        sentence = re.sub(r'\\W', ' ', sentence)\n",
    "        headline_embeddings = get_bert_embeddings(sentence)\n",
    "        text_embeddings = get_bert_embeddings(paragraph)\n",
    "        similarity = compute_cosine_similarity(headline_embeddings, text_embeddings)\n",
    "        if similarity > headline_similarity:\n",
    "            # most similar sentence\n",
    "            headline_similarity = similarity\n",
    "            headline = sentence\n",
    "    relevant_paragraphs_with_topics_similarity.append({\"headline\": headline, \"paragraph\": paragraph})\n",
    "\n",
    "print (relevant_paragraphs_with_topics_similarity)   \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b562ad6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}